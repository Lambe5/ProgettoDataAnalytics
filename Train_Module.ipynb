{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "FILENAME = \"train.csv\"\n",
    "df_res = pd.read_csv(FILENAME)\n",
    "df_res.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def my_train_test():\n",
    "    X = df_res.iloc[:, 1:]\n",
    "    y = df_res[[\"Year\"]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return(X_train, X_test, y_train, y_test)\n",
    "\n",
    "def my_train_validation_test():\n",
    "    X = df_res.iloc[:, 1:]\n",
    "    y = df_res[[\"Year\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    return(X_train, X_val, y_train, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = my_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histogram(column_name):\n",
    "    data = X_train[column_name]\n",
    "    plt.hist(data)\n",
    "    plt.show()\n",
    "\n",
    "def all_histograms(df):\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Istogramma della colonna {col}')\n",
    "        plt.xlabel('Valore')\n",
    "        plt.ylabel('Frequenza')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def boxplot_y():\n",
    "    data = y_train[\"Year\"]\n",
    "    plt.boxplot(data, whis=1.5)\n",
    "    plt.show()\n",
    "\n",
    "def density_plots(df):\n",
    "    for col in df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.displot(df[col], kind=\"kde\", fill=True)\n",
    "        plt.title(f'Densità di probabilità della colonna {col}')\n",
    "        plt.xlabel('Valore')\n",
    "        plt.ylabel('Densità')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "def plot_norm(x, x_normalized):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x_normalized)\n",
    "    plt.show()\n",
    "\n",
    "def norm(df, column_name, order):\n",
    "    x = df[column_name]\n",
    "    x_norm1 = np.linalg.norm(x, ord=order)\n",
    "    x_normalized = x / x_norm1\n",
    "    df[column_name] = x_normalized\n",
    "\n",
    "    if order == 1:\n",
    "        print(sum(x_normalized))\n",
    "    if order == 2:\n",
    "        print(sum(x_normalized**2))\n",
    "    if order == np.inf:\n",
    "        print(max(x_normalized))\n",
    "\n",
    "def min_max_sc(X_val = None):\n",
    "    #MinMax Scaling\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    min_max_scaler.fit(X_train)\n",
    "    X_train_minmax = min_max_scaler.transform(X_train)\n",
    "    X_test_minmax = min_max_scaler.transform(X_test)\n",
    "    if X_val is not None:\n",
    "        X_val_minmax = min_max_scaler.transform(X_val)\n",
    "        return (X_train_minmax, X_test_minmax, X_val_minmax)\n",
    "    else:\n",
    "        return (X_train_minmax, X_test_minmax)\n",
    "\n",
    "def standardization(X_val = None):\n",
    "    #Standardization\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    #Addestramento\n",
    "    scaler.fit(X_train)\n",
    "    #Applicazione\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    if X_val is not None:\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        return (X_train_scaled, X_test_scaled, X_val_scaled)\n",
    "    else:\n",
    "        return (X_train_scaled, X_test_scaled)\n",
    "\n",
    "X_train_norm1 = X_train.copy()\n",
    "X_train_norm2 = X_train.copy()\n",
    "X_train_normInf = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_minmax, X_test_minmax = min_max_sc()\n",
    "X_train_scaled, X_test_scaled = standardization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def f_pca(num_components = None, X_val_scaled = None):\n",
    "    if num_components is not None:\n",
    "        pca = PCA(n_components=num_components)\n",
    "    else: pca = PCA()\n",
    "    principals_components_train = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "    # Trasforma il set di test utilizzando la stessa PCA addestrata sul set di addestramento\n",
    "    principals_components_test = pca.transform(X_test_scaled)\n",
    "\n",
    "    #loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, len(X_train.columns) + 1)], index=X_train.columns)\n",
    "    #print(loadings)\n",
    "    if X_val_scaled is not None:\n",
    "        principals_components_val = pca.transform(X_val_scaled)\n",
    "        return (principals_components_train, principals_components_test, principals_components_val, pca)\n",
    "    else:\n",
    "        return (principals_components_train, principals_components_test, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principals_components_train, principals_components_test, pca = f_pca(54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_optimal_num_components(X_train_scaled):\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Plot della explained variance ratio per ogni componente\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(pca.explained_variance_ratio_, marker='o', linestyle='-')\n",
    "    plt.xlabel('Component Number')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio vs. Component Number')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(np.arange(0, len(pca.explained_variance_ratio_), step=10))\n",
    "    plt.show()\n",
    "\n",
    "    # Plot della cumulative explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0.9, color='red', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '90% cut-off threshold', color = 'red', fontsize=10)\n",
    "    plt.xticks(np.arange(0, len(pca.explained_variance_ratio_), step=10))\n",
    "    plt.show()\n",
    "\n",
    "# Usa questa funzione per trovare il numero ottimale di componenti principali\n",
    "find_optimal_num_components(X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calcola la media dei valori assoluti dei carichi per ciascun componente principale\n",
    "# mean_abs_loadings = loadings.abs().mean()\n",
    "\n",
    "# # Ordina i carichi in ordine decrescente di importanza\n",
    "# sorted_loadings = mean_abs_loadings.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Componenti Principali più importanti:\")\n",
    "# print(sorted_loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calcola gli autovalori dall'oggetto PCA\n",
    "# eigenvalues = pca.explained_variance_\n",
    "\n",
    "# # Visualizza gli autovalori\n",
    "# print(\"Autovalori dei Componenti Principali:\")\n",
    "# for i, eig in enumerate(eigenvalues, 1):\n",
    "#     print(f\"PC{i}: {eig}\")\n",
    "\n",
    "# # Puoi anche visualizzarli in un grafico a barre per una migliore comprensione della distribuzione\n",
    "# plt.bar(range(1, len(eigenvalues) + 1), eigenvalues)\n",
    "# plt.xlabel('Componente Principale')\n",
    "# plt.ylabel('Autovalore')\n",
    "# plt.title('Autovalori dei Componenti Principali')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Considerando che principals_components è un array numpy con tre colonne\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Estraiamo le colonne per l'asse x, y e z\n",
    "# x = principals_components_train[:, 0]\n",
    "# y = principals_components_train[:, 1]\n",
    "# z = principals_components_train[:, 2]\n",
    "\n",
    "# # Plot dello scatter tridimensionale\n",
    "# ax.scatter(x, y, z, c=y_train['Year'], marker='o')\n",
    "\n",
    "# # Etichette degli assi\n",
    "# ax.set_xlabel('Componente Principale 1')\n",
    "# ax.set_ylabel('Componente Principale 2')\n",
    "# ax.set_zlabel('Componente Principale 3')\n",
    "\n",
    "# plt.title('Scatter Plot dei Componenti Principali')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a threshold for which features to extract\n",
    "# threshold = 0.3\n",
    "\n",
    "# # Find features with loadings above the threshold for each principal component\n",
    "# important_features = {}\n",
    "# for column in loadings.columns:\n",
    "#     important_features[column] = loadings.index[loadings[column].abs() > threshold].tolist()\n",
    "\n",
    "# # Now 'important_features' dictionary contains the important features for each PC\n",
    "# for pc, features in important_features.items():\n",
    "#     print(f\"{pc}: {', '.join(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true,y_pred,dataset_name =\"\",model=\"\"):\n",
    "\n",
    "    return {\n",
    "            \"mae_\"+dataset_name: mean_absolute_error(y_true,y_pred),\n",
    "            \"mse_\"+dataset_name: mean_squared_error(y_true, y_pred),\n",
    "            \"mape_\"+dataset_name: mean_absolute_percentage_error(y_true, y_pred),\n",
    "            \"r2score_\"+dataset_name: r2_score(y_true, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear-Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linearReg(X_train, y_train, X_test, y_test):\n",
    "    #Linear-Regressor\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    predizioni = reg.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predizioni)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "    r_squared = r2_score(y_test, predizioni)\n",
    "    print(\"Coefficienti di determinazione R²:\", r_squared)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, predizioni)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(y_test,predizioni)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearReg(X_train, y_train, X_test, y_test)\n",
    "linearReg(X_train_minmax, y_train, X_test_minmax, y_test)\n",
    "linearReg(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "linearReg(principals_components_train, y_train, principals_components_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random-Forest-Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, X_test, y_test = my_train_validation_test()\n",
    "X_train_scaled, X_test_scaled, X_val_scaled = standardization(X_val)\n",
    "principals_components_train, principals_components_test, principals_components_val, pca = f_pca(55, X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test, n_alberi):\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_alberi, random_state=42)\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    rf_predictions = rf_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, rf_predictions)\n",
    "    r_squared = r2_score(y_test, rf_predictions)\n",
    "\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Coefficienti di determinazione R²:\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomForest(principals_components_train, y_train, principals_components_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "n_estimators_values = list(range(400,800,100))\n",
    "metrics = []\n",
    "for n_estimators in n_estimators_values:\n",
    "    modello_random_forest = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                                  max_depth= 140,\n",
    "                                                  min_samples_split= 15,\n",
    "                                                   random_state=42, n_jobs = -1)\n",
    "    \n",
    "    modello_random_forest.fit(principals_components_train, y_train)\n",
    "    \n",
    "    predizioni_val = modello_random_forest.predict(principals_components_val)\n",
    "    predizioni_test = modello_random_forest.predict(principals_components_test)\n",
    "    \n",
    "    mse_values = mean_squared_error(y_val,predizioni_val)\n",
    "    \n",
    "    metrics.append(list(calc_metrics(y_val,predizioni_val,\"val\"),calc_metrics(y_test,predizioni_test,\"test\")))\n",
    "\n",
    "# Plot dei risultati\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_values, mse_values, marker='o', linestyle='-')\n",
    "plt.title('MSE al variare del numero di alberi nel RandomForestRegressor')\n",
    "plt.xlabel('Numero di alberi')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def svr(num_fold, X_train, y_train, X_test, y_test):\n",
    "    # Definisci la griglia dei parametri da testare\n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [1, 10, 50, 100]\n",
    "    }\n",
    "\n",
    "    # Inizializza il regressore SVM\n",
    "    svm_regressor = SVR()\n",
    "\n",
    "    # Definisci il numero di fold per la cross-validation\n",
    "    num_folds = num_fold\n",
    "\n",
    "    # Inizializza il KFold\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Crea un oggetto GridSearchCV per trovare i migliori parametri con la k-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=svm_regressor, param_grid=param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Esegui la cross-validation per trovare i migliori parametri\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Stampa i migliori parametri trovati\n",
    "    print(\"Migliori parametri:\", grid_search.best_params_)\n",
    "\n",
    "    # Valuta il modello con la migliore combinazione di parametri\n",
    "    best_svm_regressor = grid_search.best_estimator_\n",
    "\n",
    "    # Valuta il modello sui dati di test\n",
    "    y_pred = best_svm_regressor.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error sui dati di test:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr(5, principals_components_train, y_train, principals_components_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(num_neig, X_train, y_train, X_test, y_test):\n",
    "    neig = KNeighborsRegressor(n_neighbors=num_neig, n_jobs=-1)\n",
    "    neig.fit(X_train, y_train)\n",
    "    y_pred = neig.predict(X_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error sui dati di test:\", mse_test)\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    print(\"Coefficienti di determinazione R²:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_cv(X_train, y_train, X_val, y_val):\n",
    "    # Definisci il range di iperparametri per il numero di vicini\n",
    "    param_grid = {'n_neighbors': np.arange(2, 21)}\n",
    "\n",
    "    # Inizializza il KNN Regressor\n",
    "    knn = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "    # Inizializza la ricerca grid\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Estrai i risultati della ricerca grid\n",
    "    train_scores_mse = np.sqrt(-grid_search.cv_results_['mean_train_score'])  # Mean Squared Error sul train set\n",
    "    test_scores_mse = np.sqrt(-grid_search.cv_results_['mean_test_score'])  # Mean Squared Error sul test set\n",
    "\n",
    "    # Inizializza la ricerca grid per MAE\n",
    "    grid_search_mae = GridSearchCV(knn, param_grid, cv=5, scoring='neg_mean_absolute_error', return_train_score=True)\n",
    "    grid_search_mae.fit(X_train, y_train)\n",
    "\n",
    "    train_scores_mae = -grid_search_mae.cv_results_['mean_train_score']  # Mean Absolute Error sul train set\n",
    "    test_scores_mae = -grid_search_mae.cv_results_['mean_test_score']  # Mean Absolute Error sul test set\n",
    "\n",
    "    neighbors = param_grid['n_neighbors'] # Numero di vicini\n",
    "\n",
    "    # Plot dell'ampliamento dell'errore al variare del numero di vicini\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #plt.plot(neighbors, train_scores, label='Train Error', marker='o')\n",
    "    plt.plot(neighbors, test_scores_mae, label='Validation Error', marker='o')\n",
    "    plt.xlabel('Number of Neighbors')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('KNN Regression - Validation Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Trova il miglior modello\n",
    "    best_neighbor = grid_search.best_params_['n_neighbors']\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Valida il miglior modello sul validation set\n",
    "    y_pred_val = best_model.predict(X_val)\n",
    "    mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "    mae_val = mean_absolute_error(y_val,y_pred_val)\n",
    "    r_squared_val = r2_score(y_val, y_pred_val)\n",
    "\n",
    "    print(\"Miglior numero di vicini:\", best_neighbor)\n",
    "    print(\"Mean Squared Error sui dati di validation:\", mse_val)\n",
    "    print(\"Coefficienti di determinazione R² sui dati di validation:\", r_squared_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv(principals_components_train, y_train, principals_components_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def prepare_data(X_train, y_train, X_val, y_val):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            #loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.numpy())\n",
    "            targets.extend(labels.numpy())\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return calc_metrics(targets,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparazione dei dati\n",
    "train_dataset, val_dataset = prepare_data(principals_components_train, y_train, principals_components_val, y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definizione del modello\n",
    "input_size = principals_components_train.shape[1]\n",
    "hidden_size = 200\n",
    "num_classes = 1\n",
    "model = FeedForward(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Definizione della loss e dell'ottimizzatore\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Addestramento del modello\n",
    "num_epochs = 10\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(evaluate_model(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardPlus(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, depth=1):\n",
    "        super(FeedForwardPlus, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        block = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        for i in range(depth):\n",
    "            model += block\n",
    "            print(\"i = \", i)\n",
    "\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparazione dei dati\n",
    "train_dataset, val_dataset = prepare_data(principals_components_train, y_train, principals_components_val, y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definizione del modello\n",
    "input_size = principals_components_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_classes = 1\n",
    "depth = 1\n",
    "\n",
    "model = FeedForwardPlus(input_size, hidden_size, num_classes, depth)\n",
    "\n",
    "# Definizione della loss e dell'ottimizzatore\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Addestramento del modello\n",
    "num_epochs = 7\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(evaluate_model(model, val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelli deep per Tabular Data:\n",
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models import TabNetModelConfig\n",
    "from sklearn.metrics import mean_absolute_error\n",
    " \n",
    "def test_regression_TabNet(\n",
    "    regression_data,\n",
    "    multi_target,\n",
    "    continuous_cols,\n",
    "    categorical_cols,\n",
    "    continuous_feature_transform,\n",
    "    normalize_continuous_features,\n",
    "    target_range,\n",
    "    batch_size=400,\n",
    "    epochs = 7\n",
    "):\n",
    "    (train, test, target) = regression_data\n",
    "    train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "    data_config = DataConfig(\n",
    "        target=target + [\"MedInc\"] if multi_target else target,\n",
    "        continuous_cols=continuous_cols,\n",
    "        categorical_cols=categorical_cols,\n",
    "        continuous_feature_transform=continuous_feature_transform,\n",
    "        normalize_continuous_features=normalize_continuous_features,\n",
    "        #num_workers=num_workers\n",
    "    )\n",
    "    model_config_params = {\"task\": \"regression\", \"metrics\":[\"mean_absolute_percentage_error\",\"mean_absolute_error\", \"r2_score\"]}\n",
    "    if target_range:\n",
    "        _target_range = []\n",
    "        for target in data_config.target:\n",
    "            _target_range.append(\n",
    "                (\n",
    "                    float(train_data[target].min()),\n",
    "                    float(train_data[target].max()),\n",
    "                )\n",
    "            )\n",
    "        model_config_params[\"target_range\"] = _target_range\n",
    "    model_config = TabNetModelConfig(**model_config_params)\n",
    "    trainer_config = TrainerConfig(\n",
    "        max_epochs=epochs,\n",
    "        checkpoints=None,\n",
    "        early_stopping=None,\n",
    "        accelerator=\"cpu\",\n",
    "        fast_dev_run=False,\n",
    "        batch_size= batch_size\n",
    "    )\n",
    "    optimizer_config = OptimizerConfig()\n",
    " \n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "    )\n",
    "    tabular_model.fit(train=train_data, validation=val_data)\n",
    " \n",
    "    result = tabular_model.evaluate(test)\n",
    "    #assert \"test_mean_squared_error\" in result[0].keys()\n",
    "    pred_df = tabular_model.predict(test)\n",
    "    assert pred_df.shape[0] == test.shape[0]\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_Tab_tests():\n",
    "    ref_df_train = pd.DataFrame(principals_components_train)\n",
    "    ref_df_test = pd.DataFrame(principals_components_test)\n",
    " \n",
    "    ref_df_train.columns = ref_df_train.columns.astype(str)\n",
    "    ref_df_test.columns = ref_df_test.columns.astype(str)\n",
    " \n",
    "    lista = list(ref_df_train.columns)\n",
    "    lista_target_range = list(range(1900, 2024))\n",
    " \n",
    "    target_column = str(ref_df_train.columns[-1])\n",
    "   \n",
    "    return (ref_df_train, ref_df_test, lista, lista_target_range, target_column)\n",
    "\n",
    "(ref_df_train, ref_df_test, lista, lista_target_range, target_column) = input_Tab_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_regression_TabNet(regression_data=(ref_df_train, ref_df_test, [target_column]), multi_target=False,\n",
    "    continuous_cols=lista,\n",
    "    categorical_cols=[],\n",
    "    continuous_feature_transform=None,\n",
    "    normalize_continuous_features=True,\n",
    "    target_range=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabTrasformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.categorical_encoders import CategoricalEmbeddingTransformer\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models import TabTransformerConfig\n",
    "\n",
    "def test_regression(\n",
    "    regression_data,\n",
    "    multi_target,\n",
    "    continuous_cols,\n",
    "    categorical_cols,\n",
    "    continuous_feature_transform,\n",
    "    normalize_continuous_features,\n",
    "    target_range,\n",
    "    epoch = 10,\n",
    "    batch_size = 500\n",
    "):\n",
    "    (train, test, target) = regression_data\n",
    "    data_config = DataConfig(\n",
    "        target=target + [\"MedInc\"] if multi_target else target,\n",
    "        continuous_cols=continuous_cols,\n",
    "        categorical_cols=categorical_cols,\n",
    "        continuous_feature_transform=continuous_feature_transform,\n",
    "        normalize_continuous_features=normalize_continuous_features\n",
    "    )\n",
    "    model_config_params = {\n",
    "        \"task\": \"regression\",\n",
    "        \"input_embed_dim\": 8,\n",
    "        \"num_attn_blocks\": 1,\n",
    "        \"num_heads\": 2,\n",
    "        \"metrics\":[\"mean_absolute_percentage_error\",\"mean_absolute_error\",\"r2_score\"]\n",
    "    }\n",
    "    if target_range:\n",
    "        _target_range = []\n",
    "        for target in data_config.target:\n",
    "            _target_range.append(\n",
    "                (\n",
    "                    float(train[target].min()),\n",
    "                    float(train[target].max()),\n",
    "                )\n",
    "            )\n",
    "        model_config_params[\"target_range\"] = _target_range\n",
    "    model_config = TabTransformerConfig(**model_config_params)\n",
    "    trainer_config = TrainerConfig(\n",
    "        max_epochs= epoch,\n",
    "        checkpoints=None,\n",
    "        early_stopping=None,\n",
    "        accelerator=\"cpu\",\n",
    "        fast_dev_run=False,\n",
    "        batch_size= batch_size\n",
    "    )\n",
    "    optimizer_config = OptimizerConfig()\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "    )\n",
    "    #print(train[53])\n",
    "    tabular_model.fit(train=train)\n",
    "\n",
    "    result = tabular_model.evaluate(test)\n",
    "    #assert \"test_mean_squared_error\" in result[0].keys()\n",
    "    pred_df = tabular_model.predict(test)\n",
    "    assert pred_df.shape[0] == test.shape[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df_train = pd.DataFrame(principals_components_train)\n",
    "ref_df_test = pd.DataFrame(principals_components_test)\n",
    "\n",
    "ref_df_train.columns = ref_df_train.columns.astype(str)\n",
    "ref_df_test.columns = ref_df_test.columns.astype(str)\n",
    "\n",
    "lista = list(ref_df_train.columns)\n",
    "lista_target_range = list(range(1900, 2024))\n",
    "\n",
    "target_column = str(ref_df_train.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_regression(regression_data=(ref_df_train, ref_df_test, [target_column]), multi_target = None,\n",
    "    continuous_cols = lista,\n",
    "    categorical_cols = [],\n",
    "    continuous_feature_transform = None,\n",
    "    normalize_continuous_features = False,\n",
    "    target_range=True, batch_size= 500, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione tabtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter(batch_size, epochs):\n",
    "    result = test_regression(regression_data=(ref_df_train, ref_df_test, [target_column]), multi_target = None,\n",
    "        continuous_cols = lista,\n",
    "        categorical_cols = [],\n",
    "        continuous_feature_transform = None,\n",
    "        normalize_continuous_features = False,\n",
    "        target_range=True,\n",
    "        batch_size=batch_size,\n",
    "        epoch=epochs )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(5,10,1))\n",
    "batch_size = list(range(100,500,100))\n",
    "results = []\n",
    "for j,batch in enumerate(batch_size):\n",
    "    for i,epoch in enumerate(epochs):\n",
    "        result = test_regression(regression_data=(ref_df_train, ref_df_test, [target_column]), multi_target = None,\n",
    "            continuous_cols = lista,\n",
    "            categorical_cols = [],\n",
    "            continuous_feature_transform = None,\n",
    "            normalize_continuous_features = False,\n",
    "            target_range=True,\n",
    "            batch_size=batch,epoch=epoch )\n",
    "        result[0][\"batch_size\"] = batch\n",
    "        result[0][\"epochs\"] = epoch\n",
    "        results.append(result)\n",
    "        print(\"batch_size: %d / %d      epoch: %d / %d\"%(j,(len(batch_size)-1),i,(len(epochs)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = [item[0] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res =pd.DataFrame(list_dict)\n",
    "df_res.to_csv(\"./df_Res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.sort_values([\"epochs\",\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(df_res['epochs'], df_res['test_mean_absolute_percentage_error'], c=df_res['batch_size'], cmap='Blues', s=100, label='loss', alpha=0.6, edgecolors='w')\n",
    "#plt.scatter(df_res['epochs'], df_res['test_mean_absolute_error'], c=df_res['batch_size'], cmap='Blues', s=200, label='MAE', alpha=0.6, edgecolors='w')\n",
    "#plt.scatter(df_res['epochs'], df_res['test_mean_absolute_percentage_error'], marker='o', label='MAPE')\n",
    "#plt.scatter(df_res['epochs'], df_res['test_r2_score'], marker='o', label='R2')\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('batch_Size')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Errore')\n",
    "plt.title('Variazione degli errori al variare delle epoche')\n",
    "plt.legend()\n",
    " \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
